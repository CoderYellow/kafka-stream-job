

status = warn

appender.console.type = Console
appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p [%t] %c{1}:%L - %m%n
appender.console.layout.type = PatternLayout
appender.console.name = consoleLogger

# Configure root logger
rootLogger.level = info
rootLogger.appenderRef.console.ref = consoleLogger
#spark default
#rootLogger.appenderRef.console.ref = console




#logger.awssdk.name: software.amazon.awssdk
#logger.awssdk.level: trace
#logger.awssdk.appenderRef.console.ref: consoleLogger
#logger.awssdk.additivity: false
#
#logger.awssdk.request.name: software.amazon.awssdk.request
#logger.awssdk.request.level: trace
#logger.awssdk.request.appenderRef.console.ref: consoleLogger
#logger.awssdk.request.additivity: false
#
#logger.awssdk.http.name: software.amazon.awssdk.http
#logger.awssdk.http.level: trace
#logger.awssdk.http.appenderRef.console.ref: consoleLogger
#logger.awssdk.http.additivity: false
#
#logger.awssdk.signer.name: software.amazon.awssdk.auth.signer
#logger.awssdk.signer.level: trace
#logger.awssdk.signer.appenderRef.console.ref: consoleLogger
#logger.awssdk.signer.additivity: false

# SHOW MICRO-BATCH EXECUTION
log4j.logger.org.apache.spark.sql.execution.streaming.MicroBatchExecution=DEBUG

# SHOW OFFSET LOGIC (reading, planning, committing)
log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG

# SHOW SOURCE OFFSET PROCESSING (Kafka)
log4j.logger.org.apache.spark.sql.kafka010.KafkaOffsetReader=DEBUG
log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=DEBUG

# SHOW CHECKPOINT WRITING
log4j.logger.org.apache.spark.sql.execution.streaming.CheckpointFileManager=DEBUG
log4j.logger.org.apache.spark.sql.execution.streaming.HDFSMetadataLog=DEBUG

# SHOW STATE STORE (if any stateful operators are used)
log4j.logger.org.apache.spark.sql.execution.streaming.state=DEBUG

# SHOW ICEBERG COMMIT STEPS
log4j.logger.org.apache.iceberg=DEBUG
log4j.logger.org.apache.iceberg.spark=DEBUG


# ---- Custom Appender (your Java class) ----
#/data/.gradle/caches/modules-2/files-2.1/org.apache.spark/spark-sql-kafka-0-10_2.13/3.5.5/18d805d88e2671001bf09e82e216456e7410f752/spark-sql-kafka-0-10_2.13-3.5.5-sources.jar!/org/apache/spark/sql/kafka010/consumer/KafkaDataConsumer.scala:425
#appender.dataLoss.type = KafkaDataLossAppender
#appender.dataLoss.name = DATA_LOSS

# =====================================================
# Loggers
# =====================================================

# Intercept Kafka data loss warnings
#logger.kafka.name = org.apache.spark.sql.kafka010.consumer
#logger.kafka.level = warn
#logger.kafka.additivity = false
#logger.kafka.appenderRefs = console, dataLoss
#logger.kafka.appenderRef.console.ref = Console
#logger.kafka.appenderRef.dataLoss.ref = DATA_LOSS